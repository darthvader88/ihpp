
\documentclass[a4paper,11pt]{report}

\author{Vladislav K. Valtchev} 
\title{IHPP: An Intraprocedural Hot Path Profiler}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{hyperref}
\usepackage{xcolor,graphicx}
\usepackage{mdwlist}
\usepackage{fix-cm}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%in order to make the analytical index
%\usepackage{makeidx}
%\makeindex

\pagestyle{headings}

\begin{document}

\thispagestyle{empty}

\begin{figure}
\centering
\includegraphics[scale=0.6]{logo}
\end{figure}


\begin{center}


{\Large\textsc{Universit\`a degli studi di Roma}\\} 
{\huge\textsc{La Sapienza}\\[10pt]}
{\huge\textsc{Facolt\`a di Ingegneria}\\[40pt]} 

{\large Tesi di laurea in: \\}
{\LARGE\textsc{Ingegneria Informatica}\\[50pt]}

{\large Docente relatore: \\}
{\large Prof. Camil Demetrescu\\[20pt]}

{\large Candidato: \\}
{\large \textbf{Vladislav K. Valtchev}\\[40pt]}

{\large Anno accademico: 2011/2012\\}


\end{center}

%in order to print the analytical index
%\printindex

\pagebreak

\thispagestyle{empty}

\begin{center}

\vspace*{4.5cm}

\fontsize{70}{90}\selectfont \textbf{IHPP}\\
\fontsize{20}{35}\selectfont
\textit{An Intraprocedural Hot Path Profiler}\\

\vspace{11cm}

\fontsize{14}{20}\selectfont
\textbf{Vladislav K. Valtchev}

\end{center}
\pagebreak

\begin{abstract}
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj
wlhrwelkjrfh sd hf kjh sdfkh jq ql lkqjhekqj q eqhkjeqkj kjahq 
erhjwe skafhd lkaerh welrk liwer  iwerh wit wielr qierhu lqr 
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj
wlhrwelkjrfh sd hf kjh sdfkh jq ql lkqjhekqj q eqhkjeqkj kjahq 
erhjwe skafhd lkaerh welrk liwer  iwerh wit wielr qierhu lqr 
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj
wlhrwelkjrfh sd hf kjh sdfkh jq ql lkqjhekqj q eqhkjeqkj kjahq 
erhjwe skafhd lkaerh welrk liwer  iwerh wit wielr qierhu lqr 
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj

\end{abstract}


\tableofcontents

\chapter{Introduction}


\section{The context (state of art)}
\section{Motivations}
\section{Contributions}
\section{Thesis structure}

\chapter{Program analysis}

Program analysis is the process of analyzing the behavior of computer programs\footnote{Program analysis: \url{http://en.wikipedia.org/wiki/Program_analysis}}. Main applications of program analysis are \emph{program correctness checking} and \emph{program optimization}.
There are two main approaches in program analysis: static and dynamic analysis.
The main difference between them is that in \emph{static} analysis nothing is executed: the analysis
is conducted only by observing the program source code or the compiled program instructions. Instead, the \emph{dynamic} program analysis is based on executing the program and observing what is it doing, even in real time if possible.

\section{Static analysis}

Static analysis can be done either by hand or by using another program. Information obtained by static analysis can be used in many ways, from highlighting possible coding errors to application of formal methods that mathematically prove the correctness of algorithms used or, in the general case, some properties. It's necessary to say, even if this isn't the right context, that, as 
theoretical computer science proved, there is no way to prove the absolute correctness of every program because of the halting problem\footnote{The halting problem: \url{http://en.wikipedia.org/wiki/Halting_problem}}.
By the way, there are a lot of methods that give us estimated solutions with a good level of reliability. We can mention four ways of doing static program
analysis\footnote{\url{http://en.wikipedia.org/wiki/Static_program_analysis}}:


\begin{description}
\item[Model checking] considers systems that have finite state or may be reduced to finite state by abstraction
\item[Data-flow analysis] is a lattice-based technique for gathering information about the possible set of values
\item[Abstract interpretation] models the effect that every statement has on the state of an abstract machine (i.e., it 'executes' the software based on the mathematical properties of each statement and declaration)
\item[Use of assertions] in program code as first suggested by Hoare logic\footnote{Hoare logic: \url{http://en.wikipedia.org/wiki/Hoare_logic}}
\end{description}

A more in deep explanation of these approaches goes too far away from the purpose of this paper.

\section{Dynamic analysis}

Dynamic program analysis is substantially done by executing the \emph{target program} in a sort of ``controlled environment''.
This description is so vague because there a lot of very different ways of doing this type of analysis as there are different objectives that who does the analysis wants to achieve. For example, it can be done in order to trace memory allocations (and discover memory leaks), to discover race conditions, memory corruption, security vulnerabilities and in general to do a \emph{performance analysis}, also known as \textbf{profiling}. We refer as \emph{profiling} when the final goal of the work is to improve the program  \emph{performance} and not, for example, to improve program \emph{correctness}. So, a memory analysis is always a dynamic analysis but isn't always a form of \emph{profiling}.

\subsection{The profiling}
The profiling is probably the most common form of dynamic program analysis and it's goal is, as just said, to analyze the performance of a program: the \emph{performance} can be the amount of memory used, the frequency of certain instructions, the frequency and/or the time consumption of some \emph{procedures} / \emph{basic blocks} inside certain procedures. Let's focus our attention on the last kind of profilers, we can classify them in two ways: according to the type of output and according to the method of data acquisition. Using the first classification rule, we get:

\begin{description}
\item[Flat profilers] \hfill \\
Profilers belonging to this kind count the number of function calls and/or average cpu time used by each function without keeping trace of the calling context of the function.
\item[Call-graph profilers] \hfill \\
These profilers do the same things that flat profilers do but they keep trace of the \emph{calling context} of a function or in the case of IHPP, also the \emph{``calling'' context} of the basic blocks (this will be explained in details later).
\end{description}

\begin{flushleft}
Instead, using the second classification method, we get:
\end{flushleft}


\begin{description}
\item[Event-based profilers] \hfill \\
Some high-level languages and frameworks have they ad-hoc profilers based on events. For example, Java has JVMPI \textit{(Java Virtual Machine Profiling Interface)}, while in .NET it's possible to attach a profiling agent as COM server to a .NET program using the \emph{Profiling APIs}. These profilers are called \emph{event-based} because statements (of the relative intermediate language) like function calls (or returns), object creations (and many others \ldots) have \emph{traps} handled at low-level by the relative virtual machine which generates \emph{events} and propagates these ones to the high-level user event-handlers objects.
\item[Statistical profilers] \hfill \\
This kind of profilers work by \emph{sampling} at regular intervals the \emph{instruction pointer} of target program through \emph{software interrupts}. This approach, of course, doesn't produce numerically exact data, but allows the program to run at near full speed. Common profilers of that kind are AMD CodeAnalyst, Apple Shark, Intel VTune and Intel Parallel Amplifier.

\item[Instrumentation profilers] \hfill \\
This kind of profilers are used for \emph{native programs}\footnote{A native program is a program written in a compiled language like C, C++, Pascal: the result of the building is an executable containing architecture-specific instructions. Instead, non-native programs (aka managed programs) don't contain binary instructions: they contain intermediate-language (IL) instructions which only the relative virtual machine understand. In order the program to run, their VM run-time compile IL instructions into machine specific instructions. \textbf{Java} and \textbf{.NET} technologies use intermediate languages called relatively \textbf{Bytecode} and \textbf{MSIL}.} and need to add binary instructions to the target program in order to ``catch events'' like function calls. Instrumentation profilers can be classified by the way they ``add instructions'' to the target program:

\begin{description}
\item[Manual]
This approach consists in modifying target program source code adding additional statements in certain locations. For example, it's possible to add profiling statements at the beginning of a set of procedures and before every their \verb|return| statement: this method of collecting data allows building function call-graphs, call context trees and much more; also, this method can be very reliable but it requires a lot of work.

\item[Automatic source level]
This approach is very similar to the last one but differs from it in the fact that profiling statements are added automatically by a tool according to an instrumentation policy.

\item[Compiler assisted]
Using a \emph{complier assisted} instrumentation means that the source code remains intact and is the compiler the one who adds profiling instructions at compile time. A practical example is \verb|gcc| when used with \verb|-pg| option: it produces an executable with profiling instructions but (in the specific case of gcc) they are executed only when the target program is executed in \emph{profiling mode} by the specific tool \verb|gprof|.

\item[Binary translation]
This approach consist of adding instructions to the already compiled binary executable.

\item[Runtime instrumentation]
In this case, the additional instructions are added at runtime after program is loaded in memory or really a little before 
they are going to be executed by the cpu. In order to this to happen, another program which has \emph{full control} of the target one is needed. This approach is used by tools like \textbf{Valgrind} and specially \textbf{Intel Pin} which is the tool used for the \textbf{IHPP} project, so it will be explained more in deep later.

\item[Runtime injection]
This technique is based on the same idea of the last one but it an a more \emph{lightweight} approach: substantially it consists of modifying the target program \emph{text} adding unconditional branch instructions to helper functions. The tool which does this work doesn't have the \emph{full control} of the target program but only partial. An example of tool which belongs to this category is \textbf{DynInst}.

\end{description} 

\item[Profiling through a hypervisor/simulator] \hfill \\
This type of profilers analyze the target program by executing it with no changes in a kind of \emph{virtual machine} which can have also some ad-hoc hardware support or it can work by literally emulating every single program instruction. This approach isn't very common today. Two historical softwares which used this approach are IBM SIMMON and IBM OLIVER.

\end{description}


\chapter{The Approach: An Intraprocedural Hot Path Profiler}


\section{Algorithms and data structures used}

\chapter{The implementation}


\chapter{Evaluation: Case studies}


\chapter{Conclusions}

\end{document}