
\documentclass[a4paper,11pt]{report}

\author{Vladislav K. Valtchev} 
\title{IHPP: An Intraprocedural Hot Path Profiler}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\usepackage{hyperref}
\usepackage{xcolor,graphicx}
\usepackage{mdwlist}
\usepackage{fix-cm}
\usepackage{array}

\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

%in order to make the analytical index
%\usepackage{makeidx}
%\makeindex

\pagestyle{headings}

\begin{document}

\thispagestyle{empty}

\begin{figure}
\centering
\includegraphics[scale=0.6]{logo}
\end{figure}


\begin{center}


{\Large\textsc{Universit\`a degli studi di Roma}\\} 
{\huge\textsc{La Sapienza}\\[10pt]}
{\huge\textsc{Facolt\`a di Ingegneria}\\[40pt]} 

{\large Tesi di laurea in: \\}
{\LARGE\textsc{Ingegneria Informatica}\\[30pt]}

{\LARGE \textbf{IHPP}\\[10pt]\textit{An \textbf{I}ntraprocedural \textbf{H}ot \textbf{P}ath \textbf{P}rofiler}}\\[50pt]


\begin{tabular*}{0.9\textwidth}{@{\extracolsep{\fill}} @{} c @{} c @{} }
{\normalsize Docente relatore: } & {\normalsize Autore:}\\
{\large \textit{Prof. Camil Demetrescu} } & {\large \textit{Vladislav K. Valtchev}}\\
\end{tabular*}

\mbox{}\\[90pt]

{\large Anno accademico: 2011/2012\\}


\end{center}

%in order to print the analytical index
%\printindex

\pagebreak

%\thispagestyle{empty}
%
%\begin{center}
%
%\vspace*{4.5cm}
%
%\fontsize{70}{90}\selectfont \textbf{IHPP}\\
%\fontsize{20}{35}\selectfont
%\textit{An \textbf{I}ntraprocedural \textbf{H}ot \textbf{P}ath \textbf{P}rofiler}\\
%
%\vspace{11cm}
%
%\fontsize{14}{20}\selectfont
%\textbf{Vladislav K. Valtchev}
%
%\end{center}
%\pagebreak

\begin{abstract}
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj
wlhrwelkjrfh sd hf kjh sdfkh jq ql lkqjhekqj q eqhkjeqkj kjahq 
erhjwe skafhd lkaerh welrk liwer  iwerh wit wielr qierhu lqr 
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj
wlhrwelkjrfh sd hf kjh sdfkh jq ql lkqjhekqj q eqhkjeqkj kjahq 
erhjwe skafhd lkaerh welrk liwer  iwerh wit wielr qierhu lqr 
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj
wlhrwelkjrfh sd hf kjh sdfkh jq ql lkqjhekqj q eqhkjeqkj kjahq 
erhjwe skafhd lkaerh welrk liwer  iwerh wit wielr qierhu lqr 
dwehkhjrkweh rlkewrhwe lkrhwek rhwel krhjweklrh welkrh wekrhwe krhwr
werwerwekr h jwekj rhwk jrhwk rhwl kjrh k kqhe qe wrkjehr r hk rhj

\end{abstract}


\tableofcontents

\chapter{Introduction}

In a world like today's one where computers are everywhere and where \mbox{Internet} has more than 2.2 billion users, software has become more important than ever. Today's operating systems have to handle much more processes than in the past and every single process is often much more ``heavy'' than before due to a lot of reasons including (but not limited to): users' demand for much more complicated tasks (for example the multimedia related ones), the intensive use of software layers by the programmers, the use of high-level interpreted languages and many others.. 
So, even if hardware's performance is still growing following Moore's law and today's computers are thousands times faster than before, they still to be ``slow'' sometimes and software still needs to be analyzed and optimized.
One way of analyzing programs' performance is \textbf{profiling}: it consists substantially in running the target program in a controlled environment and collecting data about its behavior in order to discover possible bottlenecks in software.
A basilar introduction of program analysis and various profiling techniques can be found in chapter 2.

\section{Actual profilers}

There are a lot of profiler types today and a discrete number of profilers for each category (as readers can see in the next chapter). But, \emph{most of them} have two things in common:

\begin{itemize*}
\item They focus only on procedures: data such counters and timers are collected only on the procedure basis. This means that there's no way to understand what happened \emph{inside} the procedures which caused, for example, a bottleneck.
\item Data has almost no \emph{calling context}, for example: we know that function \verb|a()| is called 10 times in our program, but we don't know who was the \emph{caller} of \verb|a()|. Some profilers like \textbf{Valgrind} can produce data with 2 levels of context: this means that we know that, for example, we know that \verb|a()| was called 10 times, 3 of them by function \verb|b()| and 7 of them by function \verb|c()|. This is better, but sometimes it isn't enough.
\end{itemize*}

Even if it isn't too difficult to collect data with full context (infinite levels) which is called ``building a \textbf{CCT}'' \emph{(calling context tree)}, this is practically useless because it grows too much specially due to function recursions: it is often too big to be physically stored and copied simply and it's too big to be human understandable. 
So, in the last years researches in theoretical computer science from all the world have proposed different approaches for building \textbf{k-levels calling contexts}. IHPP, the project described in this paper, uses one of these approaches called \textbf{kCCF} (after explained).

\section{Motivations}

The aim of creation of IHPP was to make something that allows the study of function calls with an arbitrary number of calling contexts but also to go beyond the concept of procedure-only profiling: IHPP uses the same ideas of procedure-profiling to study the execution flow of basic blocks \emph{inside} the functions. This sometimes it is really useful when there's a bottleneck in the algorithm used for solving a problem: the programmer already knows which are the \emph{slow functions} but he still doesn't understand \emph{where exactly} is the problem. Imagine functions with 3 loop levels and lots of conditional statements: it isn't trivial to understand where the problem is, neither to solve it. But, even if a profiler \emph{will not} tell the programmer \emph{how} to solve the problem, it will tell \emph{quickly} where is it, which is much better than nothing because it can save several hours of programmer's work. 

\section{Contributions}

In order to collect k-level context-sensitive data, IHPP uses some data structures and algorithms that \emph{haven't been} ideated by the author:
\begin{itemize*}
\item kSF \emph{(k-Slab Forest)}
\item kSF construction algorithm\footnote{This algorithm is used in the \textbf{traceObject()} function}
\item kCCF \emph{(k-Calling Context Forest)}
\item Forest join and inversion operations\footnote{These two operations should be intended as \emph{theoretical} operations: concrete algorithms have been developed by the author}
\end{itemize*}

For these great and innovative data structures and algorithms, the author warmly thanks \emph{Giorgio Ausiello}, \emph{Camil Demetrescu}, \emph{Irene Finocchi} and \emph{Donatella Firmani}, professors at the \emph{Sapienza University of Rome}\footnote{Officially, \textbf{Universit\`a degli studi di Roma ``La Sapienza''}}. Their work, called \emph{\mbox{k-Calling Context Profiling}}, has been accepted for the \emph{27th \mbox{ACM SIGPLAN} Conference on Object-Oriented Programming, Systems, Languages and Applications (OOPSLA 2012)}.

\section{Thesis structure}

\chapter{Program analysis}

Program analysis is the process of analyzing the behavior of computer programs\footnote{Program analysis: \url{http://en.wikipedia.org/wiki/Program_analysis}}. Main applications of program analysis are \emph{program correctness checking} and \emph{program optimization}.
There are two main approaches in program analysis: static and dynamic analysis.
The main difference between them is that in \emph{static} analysis nothing is executed: the analysis
is conducted only by observing the program source code or the compiled program instructions. Instead, the \emph{dynamic} program analysis is based on executing the program and observing what is it doing, even in real time if possible.

\section{Static analysis}

Static analysis can be done either by hand or by using another program. Information obtained by static analysis can be used in many ways, from highlighting possible coding errors to application of formal methods that mathematically prove the correctness of algorithms used or, in the general case, some properties. It's necessary to say, even if this isn't the right context, that, as 
the work of Turing, G\"{o}del and Church proved, there is no way to prove the absolute correctness of every program because of the \emph{halting problem}\footnote{The halting problem: \url{http://en.wikipedia.org/wiki/Halting_problem}}.
By the way, there are a lot of methods that give us estimated solutions with a good level of reliability. We can mention four ways of doing static program
analysis\footnote{\url{http://en.wikipedia.org/wiki/Static_program_analysis}}:


\begin{description}
\item[Model checking] considers systems that have finite state or may be reduced to finite state by abstraction
\item[Data-flow analysis] is a lattice-based technique for gathering information about the possible set of values
\item[Abstract interpretation] models the effect that every statement has on the state of an abstract machine (i.e., it 'executes' the software based on the mathematical properties of each statement and declaration)
\item[Use of assertions] in program code as first suggested by Hoare logic\footnote{Hoare logic: \url{http://en.wikipedia.org/wiki/Hoare_logic}}
\end{description}

A more in deep explanation of these approaches goes too far away from the purpose of this paper.

\section{Dynamic analysis}

Dynamic program analysis is substantially done by executing the \emph{target program} in a sort of ``controlled environment''.
This description is so vague because there a lot of very different ways of doing this type of analysis as there are different objectives that who does the analysis wants to achieve. For example, it can be done in order to trace memory allocations (and discover memory leaks), to discover race conditions, memory corruption, security vulnerabilities and in general to do a \emph{performance analysis}, also known as \textbf{profiling}. We refer as \emph{profiling} when the final goal of the work is to improve the program  \emph{performance} and not, for example, to improve program \emph{correctness}. So, a memory analysis is always a dynamic analysis but isn't always a form of \emph{profiling}.

\subsection{The profiling}
The profiling is probably the most common form of dynamic program analysis and it's goal is, as just said, to analyze the performance of a program: the \emph{performance} can be the amount of memory used, the frequency of certain instructions, the frequency and/or the time consumption of some \emph{procedures} / \emph{basic blocks} inside certain procedures. Let's focus our attention on the last kind of profilers, we can classify them in two ways: according to the type of output and according to the method of data acquisition. Using the first classification rule, we get:

\begin{description}
\item[Flat profilers] \hfill \\
Profilers belonging to this kind count the number of function calls and/or average cpu time used by each function without keeping trace of the calling context of the function.
\item[Call-graph profilers] \hfill \\
These profilers do the same things that flat profilers do but they keep trace of the \emph{calling context} of a function or in the case of IHPP, also the \emph{``calling'' context} of the basic blocks (this will be explained in details later).
\end{description}

\begin{flushleft}
Instead, using the second classification method, we get:
\end{flushleft}


\begin{description}
\item[Event-based profilers] \hfill \\
Some high-level languages and frameworks have they ad-hoc profilers based on events. For example, \textbf{Java} has \textbf{JVMPI} \textit{(Java Virtual Machine Profiling Interface)}, while in \textbf{.NET} it's possible to attach a profiling agent as COM server to a .NET program using the \emph{Profiling APIs}. These profilers are called \emph{event-based} because statements (of the relative intermediate language) like function calls (or returns), object creations (and many others \ldots) have \emph{traps} handled at low-level by the relative virtual machine which generates \emph{events} and propagates these ones to the high-level user event-handlers objects.
\item[Statistical profilers] \hfill \\
This kind of profilers work by \emph{sampling} at regular intervals the \emph{instruction pointer} of target program through \emph{software interrupts}. This approach, of course, doesn't produce numerically exact data, but allows the program to run at near full speed. Common profilers of that kind are \textbf{AMD CodeAnalyst}, \textbf{Apple Shark}, \textbf{Intel VTune} and \textbf{Intel Parallel Amplifier}.

\item[Instrumentation profilers] \hfill \\
This kind of profilers are used for \emph{native programs}\footnote{A native program is a program written in a compiled language like \textbf{C}, \textbf{C++}, \textbf{Pascal}: the result of the building is an executable containing architecture-specific instructions. Instead, non-native programs (aka managed programs) don't contain binary instructions: they contain intermediate-language (IL) instructions which only the relative virtual machine understand. In order the program to run, their VM run-time compile IL instructions into machine specific instructions. \textbf{Java} and \textbf{.NET} technologies use intermediate languages called relatively \textbf{Bytecode} and \textbf{MSIL}.} and need to add binary instructions to the target program in order to ``catch events'' like function calls. Instrumentation profilers can be classified by the way they ``add instructions'' to the target program:

\begin{description}
\item[Manual]
This approach consists in modifying target program source code adding additional statements in certain locations. For example, it's possible to add profiling statements at the beginning of a set of procedures and before every their \verb|return| statement: this method of collecting data allows building function call-graphs, call context trees and much more; also, this method can be very reliable but it requires a lot of work.

\item[Automatic source level]
This approach is very similar to the last one but differs from it in the fact that profiling statements are added automatically by a tool according to an instrumentation policy.

\item[Compiler assisted]
Using a \emph{complier assisted} instrumentation means that the source code remains intact and is the compiler the one who adds profiling instructions at compile time. A practical example is \verb|gcc| when used with \verb|-pg| option: it produces an executable with profiling instructions but (in the specific case of gcc) they are executed only when the target program is executed in \emph{profiling mode} by the specific tool \verb|gprof|.

\item[Binary translation]
This approach consist of adding instructions to an already compiled binary executable.

\item[Runtime instrumentation]
In this case, the additional instructions are added at runtime after program is loaded in memory or really a little before 
they are going to be executed by the cpu. In order to this to happen, another program which has \emph{full control} of the target one is needed. This approach is used by tools like \textbf{Valgrind} and \textbf{Intel Pin} which is the tool used for the \textbf{IHPP} project, so it will be explained in detail later.

\item[Runtime injection]
This technique is based on the same idea of the last one but it an a more \emph{lightweight} approach: substantially it consists of modifying the target program \emph{text} adding unconditional branch instructions to helper functions. The tool which does this work doesn't have the \emph{full control} of the target program but only partial. An example of tool which belongs to this category is \textbf{DynInst}.

\end{description} 

\item[Profiling through a hypervisor/simulator] \hfill \\
This type of profilers analyze the target program by executing it with no changes in a kind of \emph{virtual machine} which can have also some ad-hoc hardware support or it can work by literally emulating every single program instruction. This approach isn't very common today. Two historical softwares which adopted this approach were IBM SIMMON and IBM OLIVER.

\end{description}


\chapter{The Approach: An Intraprocedural Hot Path Profiler}


\section{Algorithms and data structures used}

\chapter{The implementation}


\chapter{Evaluation: Case studies}


\chapter{Conclusions}

\end{document}